---
title: "Package 'tmT'"
author: "author"
date: \today
output:
  pdf_document:
    toc: true
    number_sections: true
    # citation_package: ??
    keep_tex: true
---
```{r, include = FALSE}
library(devtools)
dev_mode(on = TRUE)
install_github("DoCMA-TU/tmT")
library(tmT)
dev_mode(on = FALSE)
```
\newpage
# Introduction
(TODO) how to install and so on, main target group...
```{r, eval = FALSE}
install.packages("tmT")
library("tmT")
```

# Data Preprocessing
A basic functionality of the package is data preprocessing. Therefore several functions are given for reading text data, creating text objects, manipulating these objects and especially handling duplicates of different forms in the text data.

## Read the Corpus - \texttt{textmeta}, \texttt{readWiki}
Read the corpus data through one of your self implemented read-functions and create a \texttt{textmeta} object with the function of the same name and the arguments \texttt{text}, \texttt{meta} and \texttt{metamult}. The \texttt{text} component should be a \texttt{list} of \texttt{character} vectors or a \texttt{list} of \texttt{lists} of \texttt{character} vectors, whereas \texttt{meta} should be a \texttt{data.frame} and \texttt{metamult} is intended for mainly unstructured meta-information as a \texttt{list}. Furthermore \texttt{meta} should contain columns \texttt{id}, \texttt{date} and \texttt{title}.
You can test whether your object meets the requirements of a \texttt{textmeta} object with the function \texttt{is.textmeta}.

A read-function which is part of the package \texttt{tmT} is the function \texttt{readWiki}, which is based on functionality given by the package \texttt{WikipediR}. You can handover a \texttt{category} where all pages which are associated to this category are downloaded. By default \texttt{subcategories = TRUE} are downloaded as well. The source could be specified by changing the defaults \texttt{language = "en"} and \texttt{project = "wikipedia"} analogously to \texttt{pages\_in\_category} in the \texttt{WikipediR} package. In addition to downloading the function creates a \texttt{textmeta} object. This object needs some preprocessing after reading in as well as all text data.

The function \texttt{readWiki} was used to generate the corpus, which will be used as an example corpus for further functions.
```{r, eval = FALSE}
journ = readWiki("Journalism")
bigdata = readWiki("Big data")
med = readWiki("Medicine")
corpus = mergeTextmeta(list(journ, bigdata, med))
```
The pages were downloaded on June 6, 2017 and merged to one corpus with the function \texttt{mergeTextmeta}. There are no complications expected merging the \texttt{meta data.frames}, because you will get always the same \texttt{meta} columns using \texttt{readWiki}, e.g. \texttt{date} is interpreted as the date when the page was added to a specific (sub)category. If consistency is not given the function will fill columns with \texttt{NAs} by default(\texttt{all = TRUE}) and will only return the columns which appears in all \texttt{data.frames} setting \texttt{all = FALSE}.

## Remove Umlauts and XML Tags - \texttt{removeXML}
You can use \texttt{removeXML} to delete or manipulate some characters in the \texttt{text} component of your \texttt{textmeta} object. The argument which you handover in \texttt{x} should be a \texttt{character} vector or a \texttt{list} of \texttt{character} vectors of length one. The value you receive back will be a \texttt{character} vector. If you wish to use \texttt{removeXML} on a \texttt{list} of documents with length greater than one, you should use \texttt{lapply(object, removeXML, ...)}. The function deletes XML tags if you set \texttt{xml = TRUE} (default) and it manipulates umlauts - and some other special characters - if you set \texttt{umlauts = TRUE} (default: \texttt{FALSE}) and \texttt{u.type = c("normal", "html", "all")}, which replaces umlauts through its normal forms.

As some kind of source code of Wikipedia pages the example corpus contains arrow brackets, which are typical for XML structured data. These are removed by the function \texttt{removeXML}.
```{r, eval = FALSE}
corpus$text = lapply(corpus$text, removeXML)
```
After applying the function to the corpus, it is some kind of "clean". There should nothing be in the text which has not to do with the article or page respectively itself. At this point you should deal with identifying different types of duplicates in your text data.
```{r, include = FALSE}
load("corpus.RData")
```

## Identifying Duplicates - \texttt{deleteAndRenameDuplicates}, \texttt{duplist}
You should ensure unique IDs in all three components of your \texttt{textmeta} object on your own. If you cannot ensure that, it is recommended to use the function \texttt{deleteAndRenameDuplicates}, which deletes complete duplicates - which means, that there are at least two entries with same ID and same information in \texttt{text} \textit{and} in \texttt{meta} - and renames so called "real duplicates" - at least two entries with same ID and text, but diferrent information in meta - and renames also "fake duplicates" - at least two entries with same ID but different \texttt{text} components. It is important to know that for technical reasons - expecting duplicates in the names of the \texttt{lists} - this is the only function, which works with classic indexing, so that it assumes the same order of articles in all three components.

Additionally you can identify \texttt{text} component duplicates in your corpus with the function \texttt{duplist}, which creates a \texttt{list} of different types of duplicates. Non-unique IDs are not supported by the function, which implies that \texttt{deleteAndRenameDuplicates} should be executed before.

In the given example corpus complete duplicates are only expected if pages were simultaneously associated to a category and to one of its subcategories. These duplicates are deleted. Duplicates of texts where \texttt{id} equals, but \texttt{date} differs are renamed.
```{r}
corpus = deleteAndRenameDuplicates(corpus)
```
The function \texttt{deleteAndRenameDuplicates} deleted 86 complete duplicates and renamed 227 "real" duplicates, so that \texttt{duplist} is applicable to the corpus.
```{r}
dups = duplist(corpus)
```
For further analysis, especially performing the Latent Dirichlet Allocation, it is important that for each duplicate only one page is considered. In the concret example for each case the \texttt{id} is chosen for which its page is associated first to any of the (sub)categories. These IDs including the IDs of fully unique texts are saved in the variable \texttt{myUniques}.
```{r}
earliestOfDups =
  sapply(dups$allTextDups,
    function(x) names(which.min(sapply(x,
      function(y) corpus$meta$date[corpus$meta$id == y]))))
myUniques = c(dups$notDuplicatedTexts, earliestOfDups)
```

## Clear Corpus - \texttt{makeClear}
There is a function \texttt{makeClear} for some further preprocessing of your text corpora. It removes punctuation, numbers and stopwords. By default it removes german stopwords as a extension of the function \texttt{stopwords("german")} from the \texttt{tm} package. You can control which stopwords should be removed with the argument \texttt{sw}. In addition the function changes all words to lowercase and tokenize the documents. The result is a \texttt{list} of \texttt{character} vectors, or if \texttt{paragraph} is set \texttt{TRUE} (default) a \texttt{list} of \texttt{lists} of \texttt{character} vectors. The sublists should represent paragraphs of a document. If you hand over a \texttt{textmeta} object instead of a \texttt{list} of texts you will receive one back.

The examples corpus language is english, so that \texttt{sw} should be set to \texttt{stopwords()} from the \texttt{tm} package, which includes english stopwords by default (\texttt{kind = "en"}). To create a new \texttt{textmeta} object the corresponding function is used.
```{r}
library(tm)
texts = corpus$text[names(corpus$text) %in% myUniques]
textClear = makeClear(text = texts, sw = stopwords(), paragraph = FALSE)
corpusClear = textmeta(text = textClear, meta = corpus$meta)
```

## Generate Wordlist - \texttt{makeWordlist}
When you cleared your corpus with the function \texttt{makeClear} you are also able to call the function \texttt{makeWordlist}, which creates a table of occuring words in a given corpus. For complexity reasons you can handover a parameter \texttt{k} to the function (default: \texttt{100000L}). This parameter controls how much documents should be processed at once. Large \texttt{k}s lead to faster calculation but more RAM usage.

For calculating wordlists a tokenized corpus must be used. In the given example \texttt{corpusClear\$text} is handed over to the function accordingly. 
```{r}
wordtable = makeWordlist(corpusClear$text)
```
```{r, include = FALSE}
load("corpusClear.RData")
```

# Descriptive Analysis
After preprocessing the text data there is a typical workflow we recommend of looking at the corpus. These workflow contains the generic functions \texttt{print} and \texttt{summary} as well as the highly adaptable, and following from this, powerful functions \texttt{plotScot} and \texttt{plotWord}. These graphical function should be part of every start of an analysis of text data.

## Generic Functions - \texttt{print}, \texttt{summary}
Some information about the (one to) three components of the \texttt{textmeta} object you will get by calling the generic function \texttt{print}.
```{r}
print(corpus)
```
The function provides that the count of pages in the corpus is 3458 and there are two additional columns in \texttt{meta} to the necessary ones \texttt{id}, \texttt{date} and \texttt{title}. The pages are dated from July 25, 2004 to June 6, 2017.

You will get more information, especially about counts of \texttt{NA}s and tables of some \texttt{candidates} (default: \texttt{resource} and \texttt{downloadDate}) with the generic function \texttt{summary}. In addition to \texttt{candidates} you can handover the argument \texttt{list.names} (default: \texttt{names(object)}) for specifying the components out of \texttt{text}, \texttt{meta} and \texttt{metamult} which should be analysed by the function.
```{r}
summary(corpus)
```
Apparently there are no \texttt{NA}s in the corpus as expected. The additional \texttt{meta} columns are \texttt{categoryCall} and \texttt{touched}, which are provided by \texttt{readWiki} always and contains the call of \texttt{category} in \texttt{readWiki} and the date of last revision.

## Visualisation of Corpus over Time - \texttt{plotScot}
One of the descriptive plotting functions in the package is \texttt{ploScot} which creates a plot of counts or proportion of either documents or words in a (sub)corpus over time. The subcorpus is specified by \texttt{id} and it is possible to set the \texttt{unit} to which the dates should be floored (default: \texttt{"month"}). The argument \texttt{curves = c("exact", "smooth", "both")} determine which curve(s) should be plotted. If you select \texttt{type = "words"} (default: \texttt{type = "docs"}), the object which you handover should be a cleared \texttt{textmeta} object.

First of all the complete example corpus is be plotted, as exact and smoothed curve.
```{r, fig.width=5, fig.height=2.8, fig.align=2}
plotScot(corpus, curves = "both")
```
The black curve is the exact one and the red curve represents the smoothed values. As you can see there is a peak of assignments of pages to (sub)categories between December 2012 and January 2013. This is the result of 239 pages which were assigned on January 9, 2013 to one of the subcategories of the category "Medicine". The earliest pages of the subcorpus \texttt{bigdata} were assigned on September 9, 2013.
```{r}
print(bigdata)
```
The following visualisation is one of the most important you should look at while analysing text data.
```{r, fig.width=5, fig.height=2.8, fig.align=2}
plotScot(corpus, id = myUniques, rel = TRUE)
```
The proportion of the pages from these which IDs are in \texttt{myUnique} is very useful for identifying time effects concerning duplicates. In the wikipedia corpus there are some drops for example on October 2011 with $0.79$ and on July 2015 with $0.83$. The zeros from August to November of 2004, January and March of 2005 and May of 2006 results from no articles in the whole corpus during these time periods. It is possible to get these values as \texttt{NAs} by setting \texttt{natozero = FALSE}. This only has an effect if \texttt{rel = TRUE} and is offered by the functions \texttt{plotWord}, \texttt{plotTopic} and \texttt{plotTopicWord}, too.

## Frequency Analysis - \texttt{plotWord}
The other descriptive plotting function is \texttt{plotWord} which performs a frequency analysis. Most of the arguments does not differ from \texttt{plotScot}. But the options \texttt{wordlist} and \texttt{link = c("and", "or")} are added for specifying the words of the frequency analysis and their link within one vector. In detail \texttt{wordlist} could either be a \texttt{list} of \texttt{character} vectors or a single \texttt{character} vector, which will be coerced to a \texttt{list} of the vectors length. Each \texttt{list} entry represents a set of words which all (default \texttt{link = "and"}) or one of them (\texttt{link = "or"}) should appear in a article to be counted. The function uses \texttt{subcorpusWord} with \texttt{out = "count"}, which is explained later on, for counting.

The example corpus contains pages from Wikipedia concerning the categories (and their subcategories) "Journalism", "Medicine" and "Big data". Therefore some typical words out of these categories were taken to perform a frequency analysis. First of all the words \textit{hospital}, \textit{pill} and \textit{drug} were taken.
```{r, fig.width=5, fig.height=3.5, fig.align=2, fig.show="hold"}
wordsMed = list("hospital", "pill", "drug", c("hospital", "pill", "drug"))
plotWord(corpusClear, wordlist = wordsMed, curves = "smooth",
  ylim = c(0, 20), legend = "topleft")
plotWord(corpusClear, wordlist = wordsMed, link = "or", curves = "smooth",
  ylim = c(0, 20), legend = "topleft")
```
In the figures above you can see the difference between the \textit{and} link and \textit{or} link. The three curves indicating the single words rises to the end of observation time and seem proprtional to the observed pages over time. The word \textit{pill} appears much less than the other two words and for this reason the curve of all three words together, which always has to lie beneath the lowest single curve if \texttt{link = "and"}, is zero on nearly every observation point. In the second figure the same single curves are shown. The fourth curve represents all three words again, but setting \texttt{link = "or"}. Off course the curve has to lie above all three single curves at every point. This is given except in one point which is caused by smoothing the curves. The curve lies above the three others in every point if you choose \texttt{curves = "exact"}. While the effect of the word \textit{pill} is marginal for the cumulated curve it is remarkable that the intersect between pages, which contains the word \textit{hospital} or \textit{drug}, is not as high as one could expect.

In another figure the counts of pages in which the words \textit{newspaper} and \textit{magazine} or \textit{scale}, \textit{large} and \textit{computer} respectively appears, are analysed.
```{r, fig.width=5, fig.height=3, fig.align=2}
wordsOthers = list(c("newspaper", "magazine"), c("scale", "large", "computer"))
plotWord(corpusClear, wordlist = wordsOthers, link = "or", curves = "both",
  both.lwd = 2, legend = "topleft")
```
With setting \texttt{curves = "both"} you can see the event on January 2013, like in the first figure of \texttt{plotScot}, in the light green curve. This shows that the event is not limited to pages from the category "Medicine". Therefore the event could have to do with some database structuring of Wikipedia at these dates or maybe the second curve which should specify words from the category "Big data" includes pages from the category "Medicine". The bigger and deeper colored curves represents the smoothed values. Especially the green curve increases for later dates. This fits to the expectation of "Big data" as a more modern category than the other ones, but nothing more. There is no chance of validation due to this figure.

The functions \texttt{plotScot} and \texttt{plotWord}, and usually all other plot functions in the package \texttt{tmT}, returns the table which is plotted as invisible output. Both functions offer a lot more functionality, which cannot be displayed in detail.

## Write CSV Files - \texttt{showArticles}, \texttt{showMetadata}
There are two functions for writing csv files implemented in the \texttt{tmT} package. Any of both needs an any-formated \texttt{textmeta} object in \texttt{showArticles}, respectively the \texttt{meta} component of any-formated \texttt{textmeta} object in \texttt{showMetadata}, so you can even handover a \texttt{textmeta} object with tokenized documents to \texttt{showArticles}. The default of the parameter \texttt{id} in \texttt{showArticles} are all document IDs of the corpus as a \texttt{character} vector, but it is possible to handover a \texttt{character} matrix as well, so that each column will be represented in its own csv file. In the first column of the csv file there will be the ID of each document, in the second and third the title and the date, whereas in the fourth column there will be the text itself.

Six IDs are sampled from the whole corpus. These pages are saved as \texttt{corpus1lesen.csv} and are returned as invisible to \texttt{temp}.
```{r}
set.seed(123)
id = sample(corpus$meta$id, 6)
temp = showArticles(corpus, id = id)
temp[, c("id", "date", "title")]
```
These six pages contain a "real" duplicate. You can identify them such fast looking at the column \texttt{id}. The six titles seems to lead to pages which are from different categories. Therefore you can have a look at the meta data.

The default of the parameter \texttt{id} in \texttt{showMetadata} are the IDs which are in the column \texttt{meta\$id}. You can also handover a matrix of IDs like in \texttt{showArticles} and you can specify which columns of the \texttt{meta} component you want to be written in the csv by setting the argument \texttt{cols} (default: \texttt{colnames(meta)}).

Analogously to \texttt{showArticles} the following code example will create three files named \texttt{corpus<i>meta.csv}, where $i = 1,2,3$ stands for the $i$-th column of the matrix of IDs.
```{r}
temp = showMetadata(corpus$meta, id = matrix(id, nrow = 2),
  cols = c("title", "categoryCall", "touched"))
temp
```
The function shows that the sampled pages are from all three category calls "Medicine", "Journalism" and "Big data". Five out of six pages were touched in the last month before downloading.

# Further Preparation
The preprocessing which was done before is kind of mandatory. For further preparation the package offers functions for filtering the corpus by dates or some words to generate subcorpora. In addition a function for preparing your corpus for performing a Latent Dirichlet Allocation is given. This function creates a object which can be handed over to the function you could use for a LDA.

## Filter Corpus by Dates - \texttt{subcorpusDate}
There are two implemented ways to filter your text corpora: One of these is the function \texttt{subcorpusDate}, which filters a given \texttt{textmeta} object by a time period. The function works on any formated objects of \texttt{class} \texttt{textmeta} and extracts documents out of the \texttt{text} component, from which the date column in the \texttt{meta} component is in between \texttt{s.date} and \texttt{e.date} - both including. The return value is the filtered \texttt{textmeta} object or a \texttt{list} which could be the \texttt{text} component of a \texttt{textmeta} object respectively, if you hand over the \texttt{text} and \texttt{meta} component not as a \texttt{textmeta} object.

The example corpus is filtered to pages which are associated first between 2011 and 2016.
```{r}
corpusDate = subcorpusDate(corpusClear, s.date = "2011-01-01", e.date = "2016-12-31")
print(corpusDate)
```
The filtered corpus contains 2353 pages.

## Filter Corpus by Words - \texttt{subcorpusWord}
The use of \texttt{subcorpusWord} works analogously. It filters the \texttt{text} component of a \texttt{textmeta} object by appearances of specific words. The function uses regular expressions and is very powerful by this. It filters the given documents in the \texttt{text} component by some words handed over by \texttt{search}, which could be a simple \texttt{character} vector or a \texttt{list} of \texttt{data.frames}. In the first case the entries of the vector are linked by an \textit{or}, so \textit{any} of the words must appear in one specific document for it to be returned. In the second case each \texttt{data.frame} is linked by an \textit{or} and should contain columns \texttt{pattern} including the search terms, \texttt{word} and \texttt{count}. The column \texttt{word} is a logical variable which controls whether words (\texttt{TRUE}) or substrings are searched. Alternatively \texttt{word} can be a \texttt{character} string containing the keyword \texttt{left} or \texttt{right} for left- or right-truncated search. You must set the argument \texttt{count} to an \texttt{integer}. As you can imagine this argument controls how often a word or substring must appear in a document for it to be returned. Rows in each \texttt{data.frame} are linked by an \textit{and}. Examples for the \textit{or} or \textit{and} link respectively are given by the next code example.

Maybe you are not interested in the texts of the documents itself. Therefore you can set \texttt{out} to control the output: By default (\texttt{out = text}) you get the filtered documents or if you hand over the argument \texttt{object} the corresponding \texttt{textmeta} respectively back. If you choose \texttt{out = bin} you get the corresponding logical vector of indices and if you choose \texttt{out = count} you get a matrix - with the number of documents rows and the \texttt{search}-length respectively vector-length columns - which indicates in row \textit{i} and column \textit{j} how often the \textit{j}-th word of the \texttt{wordlist} appears in the according \textit{i}-th document.
```{r}
texts = list("schaafdung", "anything")
words = c("schaaf", "afd", "dung", "anything")
subcorpusWord(text = texts, search = words, out = "bin")
```
The returned values are \texttt{TRUE} twice. There is at least one word in \texttt{words} which appears at least once in each of the strings \textit{schaafdung} and \textit{anything}.
```{r}
wordframe = data.frame(pattern = words, word = FALSE, count = 1)
subcorpusWord(text = texts, search = wordframe, out = "bin")
```
In the case that \texttt{words} is handed over as \texttt{data.frame}, the \textit{and} link is active. The function checks whether all of the words appears as part of words in the two entries of \texttt{texts}. Therefore the function returns \texttt{FALSE} twice.
```{r}
subcorpusWord(text = texts, search = wordframe[1:3,], out = "bin")
```
If you omit the word \textit{anything} from \texttt{words} you receive a \texttt{TRUE} for \textit{schaafdung} - all three words appears in it - and a \texttt{FALSE} for \textit{anything}, because not all words appears in it, not even one of them.

An example of \texttt{out = count} is given by the following.
```{r}
subcorpusWord(text = list(c("i", "was", "here", "text"),
  c("some", "text", "about", "some", "text", "and", "something", "other")),
  search = c("some", "text"), out = "count")
```

In the case of \texttt{out = count} it is useful, that \texttt{search} is a simple \texttt{character} vector. Another application of \texttt{subcorpusWord} is to apply the function with \texttt{word = TRUE}, so that the function searchs only for single words, not for strings containing these words. This is displayed by the following example.
```{r}
texts = list("land and and", c("and", "land", "and", "and"))
term = data.frame(pattern = "and", word = c(TRUE, FALSE), count = 1)
subcorpusWord(text = texts, search = split(term, term$word), out = "count")
```
The function returns counts \texttt{c(3, 4)} for the simple search of substrings and \texttt{c(2, 3)} for the restricted word search, cause the word \textit{and} appears once in every document of \texttt{texts} only as substring and not as own word.

The example corpus is filtered to those pages which fit to the chosen categories sofar, that the name of one of the catagories should appear on the page at least once, even in a substring.
```{r}
words = list(
  data.frame(pattern = "journalism", word = FALSE, count = 1),
  data.frame(pattern = c("big", "data"), word = FALSE, count = 1),
  data.frame(pattern = "medicine", word = FALSE, count = 1))
corpusFiltered = subcorpusWord(corpusDate, search = words)
print(corpusFiltered)
```
The date and word filtered corpus consists of 1460 pages.

## Transform Corpus - \texttt{docLDA}
The next and last step before performing a Latent Dirichlet Allocation is to create corpus data, which could be handed over to the function \texttt{lda.collapsed.gibbs.sampler} from the \texttt{lda} package or the function \texttt{LDAstandard} from this package respectively. This is gained by using the function \texttt{docLDA} with its arguments \texttt{corpus} and \texttt{vocab} which expect a \texttt{text} component of a \texttt{textmeta} object and a \texttt{character} vector of vocabularies. These vocobularies are the words which are taken into account for LDA. The function offers options \texttt{ldacorrect}, \texttt{excludeNA} and \texttt{reduce} set all \texttt{TRUE} by default. The returned value is a \texttt{list} in which every entry symbolies a article and contains a matrix with two rows. In the first row there is the index of the word in \texttt{vocab} minus one, in the second row there is the number of appearances of the word in the article. The option \texttt{ldacorrect = TRUE} ensures the second row is always one and the number of the appearances of the word will be shown by the number of columns belonging to this word.

Looking at the example corpus at first a new wordlist must be generated based on the filtered corpus.
```{r}
wordtableFiltered = makeWordlist(corpusFiltered$text)
```
```{r, eval = FALSE}
sortedWords = sort(wordtableFiltered$wordtable, decreasing = TRUE)
head(sortedWords)
```
```{r, echo = FALSE}
sortedWords = sort(wordtableFiltered$wordtable, decreasing = TRUE)
a = head(sortedWords)
names(a)[1] = "-"
print(a)
```
The most often "word" which appears in the filtered corpora is the sign "-" with a count of 15431, so that it appears more than nine times on a page in mean. The second most often word is \textit{edit}. Maybe this word and \textit{retrieved} should be handled as a stopword, as well "-" as word is discussable. Nevertheless these words are also considered for analysis.
```{r}
words5 = sort(names(sortedWords)[sortedWords > 5])
pagesLDA = docLDA(text = corpusFiltered$text, vocab = words5)
```
After receiving the words which appears at least six times in the whole filtered corpus, the function \texttt{docLDA} is applied to the example corpus with \texttt{vocab = words5}. The object \texttt{pagesLDA} will be handed over to the function which performs a Latent Dirichlet Allocation.

# Latent Dirichlet Allocation
The main analytical functionality requested by text mining tools is to perform and analyse a Latent Dirichlet Allocation. In the package \texttt{tmT} this is ensured by the function \texttt{LDAstandard} for performing the LDA, functions for validating the LDA results and various functions for visualize the results in different ways, especially over time. It is possible to analyse individual articles and its topic allocations as well.

## Performing LDA - \texttt{LDAstandard}
The function which has to be applied first to the corpus manipulated by \texttt{docLDA} is \texttt{LDAstandard}. Therefore the function offers the options \texttt{K} (\texttt{integer}, default: \texttt{K = 100}) to set the number of topics, \texttt{vocab} (\texttt{character} vector) for specifying the words which are considered in the manipulation of the corpus and several more e.g. number of iterations for the burnin (default: \texttt{burnin = 70}) and the number of iterations for the gibbs sampler (default: \texttt{num.iterations = 200}). The result will be saved in a \texttt{R} workspace, the first part of the results name can be specified by setting the option \texttt{folder} (default: \texttt{folder = "lda-result"}).

In the concrete example corpus the manipulated corpus \texttt{pagesLDA} is used for \texttt{documents}, the topic number is set to \texttt{K = 10} and for reproducibility a seed is set to \texttt{seed = 123}. The filename consist of the \texttt{folder} argument followed by the options of \texttt{K}, \texttt{num.iterations}, \texttt{burnin} and the \texttt{seed} of the LDA.
```{r, eval = FALSE}
LDAstandard(documents = pagesLDA, K = 10L, vocab = words5, seed = 123)
load("lda-result-k10i200b70s123.RData")
```
```{r, include = FALSE}
load("lda-result-k10i200b70s123.RData")
```
For validation of the LDA result and further analysis, the result is loaded back to workspace.

## Validation of LDA Results - \texttt{intruderWords}, \texttt{intruderTopics}
For validation of LDA results there are two functions in the package. These functions expect user input, the user works like a text labeller. The LDA result is handed over by setting \texttt{beta = result\$topics}. During the function \texttt{intruderWords} the labeller gets a set of words. The number of words can be set by \texttt{numOutwords} (default: 5). These set represents one topic. It includes a number of intruders (default: \texttt{numIntruder = 1}), which can also be zero. In general, if the user identify the intruder(s) correctly this is a identifier for a good topic allocation. You can set options \texttt{numTopwords} (default: 30) to control which top words of each topic are considered for this validation. In addition it is possible to enable or disable the possibility for the user to mark nonsense topics. By default this option is enabled (\texttt{noTopic = TRUE}). The true intruder can be printed to the console after each step with \texttt{printSolution = TRUE} (default: \texttt{FALSE}).

The LDA result of the example corpus is checked by \texttt{intruderWords} with a number of intruders of zero or one.
```{r, eval = FALSE}
set.seed(101)
intWords = intruderWords(beta = result$topics, numIntruder = 0:1)
```
```{r, include = FALSE}
set.seed(101)
intWords = intruderWords(beta = result$topics, numIntruder = 0:1,
  test = TRUE, testinput = as.character(c(5,0,0,5,1,2,0,5,3,5)))
```
```{r, echo = FALSE}
set.seed(101)
toDelete = intruderWords(beta = result$topics, numIntruder = 0:1,
  test = TRUE, testinput = "q", printSolution = TRUE)
```
By way of illustration the first set is shown. Obviously the word \textit{newspaper} does not fit into the set with the words \textit{computational}, \textit{cell}, \textit{cells} and \textit{engineering}. Therefore the user would type \texttt{5} and press enter. If the user want to mark nonsense topics he would type \texttt{x} and \texttt{0} if he thinks there is no intruder word. Actually \textit{newspaper} is the true intruder in the set above. As an example user input \texttt{c(5, 0, 0, 5, 1, 2, 0, 5, 3, 5)} is considered.
```{r}
print(intWords)
```
By printing the object of \texttt{intruderWords} to the console, you get information about options for the validation strategy and a results matrix with ten rows an three columns. The rows indicate the different sets of potential intruders. For each set the matrix contains information how much intruder are in the specific set, how much intruders were missed by the user and how much false intruders were named. Certainly the columns \texttt{missIntr} und \texttt{falseIntr} matchs if \texttt{numIntruder} is a scalar and the user names exactly this number of potential intruders for each set.
```{r}
summary(intWords)
```
Applying \texttt{summary} to an object of type \texttt{intruderWords} will result in an ouput of some measures concerning the validation. Each function call contains ten sets. You are able to continue labelling by calling \texttt{intruderWords} with \texttt{oldResult = intWords} if your set was not finished. 
```{r, eval = FALSE}
intWords = intruderWords(oldResult = intWords)
```

Analogously to \texttt{intruderWords} you can use \texttt{intruderTopics} for validation the other way around. This function is used for validation of topics associated to a specific document instead of validation of words associated to one topic. Therefore the document is displayed in another window and a sample of topics - represented by the ten \texttt{top.topic.words} - is shown in the console. You should hand over in \texttt{text} the text component of the original untokenized corpus before manipulation by \texttt{makeClear}, so that the document is kind of readable. The user then is used to name the intruder(s). There are options for different numbers of topics and intruders like in \texttt{intruderWords} as well. The option \texttt{theta} should be set to \texttt{result\$document\_expects} if in \texttt{result} the LDA result is saved. An example call is given below.
```{r, eval = FALSE}
intruderTopics(text = corpus$text, id = ldaID,
  beta = result$topics, theta = result$document_expects)
```

## Clustering of Topics - \texttt{clusterTopics}, \texttt{mergeLDA}
For analysing topic similarities and looking for the right topic number for performing further LDA it is useful to cluster the topics. The function \texttt{clusterTopics} implements this. The main argument is \texttt{topics} and should be set to the \texttt{topics} element of your \texttt{result} object. You could specify \texttt{file}, \texttt{width} and \texttt{height} (both \texttt{integers}) to write the resulting plot to a pdf. Other options are \texttt{topicnames} for labelling the topics in the plot and \texttt{method} (default: \texttt{"average"}), which influences the way the topics are clustered. The \texttt{method} statement is used for applying the distance matrix tothe function \texttt{hclust}. The distance matrix is computed based on the hellinger distance and is returned in a list together with the value of the \texttt{hclust} call as invisible by \texttt{clusterTopics}.

```{r, fig.width=5, fig.height=3.3, fig.align=2}
clustRes = clusterTopics(ldaresult = result, xlab = "Topic", ylab = "Distance")
names(clustRes)
```
The same plot as above can be recreated by calling \texttt{plot(clustRes\$cluster)}. In the plot you can see the similarities concerning the hellinger distance of the topics. Maybe it is of interest to look deeper at similar topics. In this example you could look at the topics 4 and 5.
```{r}
library(lda)
top.topic.words(result$topics[4:5, ], num.words = 6)
```
The top six representative words of the topics 4 and 5 do not look like they come from similar topics. This can be a side effect of the example corpus which is only a set of articles from three different categories in wikipedia, but ten topics were choosen for the LDA. There are also many words in the corpus which should be deleted in preprocessing like \textit{edit} and one letter words.

It is possible to merge different results of LDAs by calling \texttt{mergeLDA(list(result1, result2, ..., resultN))}. The function \texttt{mergeLDA} binds the \texttt{topics} elements of the results by row and only consider words which appears in all results. As result you receive the \texttt{topics} matrix including all topics from the given results.

## Visualisation of Topics over Time - \texttt{plotTopic}
As extension of the highly flexible functions \texttt{plotScot} and \texttt{plotWord} the package \texttt{tmT} offers at least one more plotting function of the same type. The function \texttt{plotTopic} does something very similar to these two functions. It pictures the counts or proportion of words allocated to different topics of a LDA result over time. The result object is handed over in \texttt{ldaresult}, the belonging IDs of documents as a \texttt{character} vector in \texttt{ldaid}. In \texttt{object} the function expect a strictly tokenized \texttt{textmeta} object. You could set \texttt{select} for selecting topics by an \texttt{integer} vector. By default all topics are selected. Analoguesly to \texttt{wnames} in \texttt{plotWord} it is possible to set topic names with \texttt{tnames}. By default the index and the most representative word (\texttt{top.topic.words}) per topic are chosen as names. For further individualisation the function offers mostly the same options like \texttt{plotScot} and \texttt{plotWord}.

Often it is useful to choose \texttt{curves = "smooth"} if you do not select topics, because there is a massive fluctuation of exact curves.
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
plotTopic(object = corpusFiltered, ldaresult = result, ldaID = ldaID,
  rel = TRUE, curves = "smooth", smooth = 0.1, legend = "none", ylim = c(0, 0.35))
```
There are not less than three topics with clear peaks at the first quarter of the years 2012, 2014 and 2015. At the beginning of year 2013 there is also a small peak of another topic. This is a good example for identifying periods of time where reporting could be less diversely. In the given example it is cumbersome trying to find reasons for the peaks.

There is no difference of handing over an inflated corpus with documents which were not used for LDA. But the corpus should contain all documents of the LDA.
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
plotTopic(object = corpusClear, ldaresult = result, ldaID = ldaID,
  select = c(1:2, 7:8), rel = TRUE, curves = "both", smooth = 0.1)
```
In the graphic above you could see, that the the topic \textit{medicine} is nearly time independant. However the topic \textit{news} has a peak on the first half of the year 2012, the topic \textit{syndrome} on the first quarter of 2014. The topic \textit{data} is less represented. Therefore the peak at the end of 2013 is considerable. The light colors displays the exact curves. Obvoiusly they alternate irrregular. It is important to have a look at the exact curves, because the smoothed curves are someway manipulated by the statement \texttt{smooth}, so the user is tempted to optimise the smoothing parameter for getting the curves he or she wants.

## Visualisation of Topic Share over Time - \texttt{sedimentPlot}
The function \texttt{sedimentPlot} offers possibilities to create so called sediment visualisations of topics over time. It requires arguments \texttt{ldaresult}, \texttt{ldaid} and \texttt{meta} as introduced before. There are options \texttt{select}, \texttt{tnames}, \texttt{unit} and others. Additionally you can set \texttt{threshold} to a \texttt{numeric} between 0 and 1, as a limit, which a topics proportion have to surpass at least once to be plotted.

As this seems to be interesting topics \textit{T1.syndrome} (red curve), \textit{T3.health} (green) and \textit{T7.medicine} (blue) are plotted in a sediment plot. The chosen \texttt{unit} is \texttt{"bimonth"}.
```{r, eval = FALSE}
sedimentPlot(ldaresult = result, ldaID = ldaID, meta = corpusFiltered$meta,
  select = c(7, 3, 1), unit = "bimonth", sort = FALSE)
```
```{r, echo=FALSE, fig.width=5.5, fig.height=3, fig.align=2}
par(mar = c(3.5,3,1.5,1.5)+0.1)
sedimentPlot(ldaresult = result, ldaID = ldaID, meta = corpusFiltered$meta,
  select = c(7, 3, 1), unit = "bimonth", sort = FALSE)
# evtl legend optimieren, main, xlab, ylab
# evtl plotSediment statt sedimentPlot
```
(TODO) Interpretation.

## Visualisation of Words in Topic over Time - \texttt{plotTopicWord, plotWordpt}
Another visualisation possibility of topics over time is given by \texttt{plotTopicword}. It displays the counts or proportion of given topic-word combinations. If \texttt{rel = TRUE} the baseline for normalisation are the words counts, not the counts of topics. Arguments which has to specified are \texttt{object} (corpus, \texttt{textmeta} object), \texttt{docs} (corpus manipulated by \texttt{docLDA}, the input for \texttt{LDAstandard}) and the \texttt{ldaresult} with its \texttt{ldaid} (IDs of documents in \texttt{docs} or \texttt{ldaresult} respectively). The function asks for \texttt{docs} for complexity reasons. The certain object should be created while preparation for LDA anyway. The options \texttt{wordlist} and \texttt{select} are known from other plot functions and offers a lot of different topics words combinations which should be plotted by \texttt{plotTopicword}.

In the example corpus the proportion of the word \textit{medical} in the topics one, three and seven is explored. The chosen word is the fifth most frequently word in the filtered corpus. The \texttt{top.topic.words} of the three chosen topics are \textit{syndrome} (lightgreen curve), \textit{health} (orange) and \textit{medicine} (purple).
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
plotTopicWord(object = corpusFiltered, docs = pagesLDA, ldaresult = result, ldaID = ldaID,
  wordlist = "medical", select = c(1, 3, 7), rel = TRUE, legend = "none")
```
The graphic shows that the word \textit{medical} is associated to the topic \textit{health} most often. Drops of the orange curve goes with peaks of the purple curve (\textit{medicine}). Aspects like this can be find easily with \texttt{plotTopicWord} and should lead to further analysis of the corresponding dates. The little peak of the topic allocations at January 2014 does not surprise because the topic itself has a massive peak at this date which you could see in the graphic before.

For interpretating it is important to keep in mind the baseline, the word counts of \textit{medical}. To display this the sums of all topic-word proportions are calculated and are expected to be one for all dates.
```{r, eval = FALSE}
tab = plotTopicWord(corpusFiltered, pagesLDA, result, ldaID, "medical", rel = TRUE)
all(round(rowSums(tab[, -1]), 10) == 1)
```
```{r, include = FALSE}
tab = plotTopicWord(corpusFiltered, pagesLDA, result, ldaID, "medical", rel = TRUE)
all(round(rowSums(tab[, -1]), 10) == 1)
```
```{r, echo = FALSE}
all(round(rowSums(tab[, -1]), 10) == 1)
```
This is confirmed by the call above. For some analysis maybe it could be interesting to take the other possible baseline, the topic counts, into account. Therefore there is an additional function called \texttt{plotWordpt}.

The function \texttt{plotWordpt} works analogously like its pendant \texttt{plotTopicWord}, but with baseline topic sums instead of word sums. The difference between both functions \texttt{plotWordpt} and \texttt{plotTopicWord} is given by the fact that \texttt{plotWordpt} considers topic peaks. You will get the relative counts of the selected word(s) in the selected topic(s). Obviously all curves sum up to one if you choose any topic and the whole vocabulary list as wordlist.
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
plotWordpt(object = corpusFiltered, docs = pagesLDA, ldaresult = result, ldaID = ldaID,
  wordlist = "medical", select = c(1, 3, 7), rel = TRUE)
```
The plot indicates some peaks of the word \textit{medical} in the topic \textit{T3.health} especially in the middle of the years 2011, 2012 and 2014. The proportions of \textit{medical} in the other two topics do not differ from zero often. In the end of 2011 and the middle of 2013 the topic proportion of the word increases in both topics, but do not reach five percent of the topic words in the given months.

## Heatmap of Topics over Time including Clustering - \texttt{totHeat}
Beside the plot functions using \texttt{Base R} there is one function using functionality from \texttt{gplots}. The function is called \texttt{totHeat}. Maybe there will be an opportunity to implement this function in \texttt{Base R} in later updates. The use case for \texttt{totHeat} is given by searching for explicit peaks of coverage of some topics. Therefore the resulting heatmap shows the deviation of the proportion of a given topic at this current time from its mean proportion. In addition a dendrogramm is plotted on the left side of the heatmap showing similarities of topics. The clustering is performed with \texttt{hclust} on the dissimilarities computed by \texttt{dist}.

By default the  proportions are calculated on the article lengths, but it is possible to force calculation on only the LDA vocabulary by setting \texttt{object} to a \texttt{textmeta} object only including meta information. Otherwise a strictly tokenized \texttt{textmeta} object is required. The parameters \texttt{ldaresult} and \texttt{ldaid} expect a LDA result and according IDs like in functions mentioned before. Options \texttt{tnames} (topic label), \texttt{file} (if you want to save the plot in a pdf) and \texttt{unit} (default: round dates to \texttt{"year"}) are given as well. Additionally it is possible to set whether the deviations should be normalised to take different topic sizes into account (default: \texttt{norm = FALSE}). You can change the intervals of labeling on the x-axis by setting \texttt{date\_breaks}. By default (\texttt{date\_breaks = 1}) every label is drawn. If you choose \texttt{date\_breaks = 5} every fifth label will be drawn.

The peak of the topic \textit{T1.syndrome} in January 2014 was mentioned several times before. This should be visible in the following heatmap as well. As compromise between clarity and interpretability \texttt{unit = "quarter"} is chosen. 
```{r, fig.height=6, fig.width=10}
totHeat(object = corpusFiltered, ldaresult = result, ldaID = ldaID, unit = "quarter")
```
As expected the \textit{T1.syndrome} topics peak is clearly identifiable. The according rectangle at the first quarter of 2014 is colored by the deepest red of this figure. On the other hand mostly all other quarters of years concerning this topic are colored lightblue. Other remarkable quarters are for example the fourth quarter of 2015 or 2016, where the topic \textit{T2.data} or \textit{T4.cells} respectively has noticeable peaks. The dendrogramm shows that none of the topics are similar to another concerning the absolute deviations of topic proportion from the mean topic proportion per quarter. This approves the findings of clustering the topics with \texttt{clusterTopics}.

## Individual Cases Contemplation - \texttt{topArticles}, \texttt{topicsInText}
For some reason it is useful to look at some individual cases sometimes. Especially the documents with the highest counts or proportion of words belonging to one topic are of interest. These documents can be extracted by \texttt{topArticles}. By default (\texttt{rel = TRUE}) the proportion is considered. The function requires a \texttt{ldaresult} and the according \texttt{ldaid}. It offers options \texttt{select}, \texttt{limit} and \texttt{minlength}, which control how much articles per topic (default: all topics) are given back (default: \texttt{limit = 20}) and articles of which minimum length (default: \texttt{minlength = 30}) are taken into account. The output value is a matrix of the according IDs.

In the example the top four pages from the topics \textit{T1.syndrome}, \textit{T3.health} and \textit{T7.medicine} are requested.
```{r}
topID = topArticles(ldaresult = result, ldaID = ldaID, select = c(1, 3, 7), limit = 4)
dim(topID)
```
Obviously the corresponding matrix has four rows and three columns.

After identifying the top pages it is possible to have a deeper look at these articles. Therefore the mentioned function \texttt{showArticles} can be used. The returned value is a list with three entries with \texttt{data.frames} of four rows - the different pages - and four columns each - \textit{id}, \textit{title}, \textit{date} and \textit{text}. For displaying, the fourth column of each \texttt{data.frame} containing the pages content itself is removed.
```{r}
topArt = showArticles(corpusFiltered, id = topID)
lapply(topArt, function(x) x[, 1:3])
```
The top three pages from topic \textit{T1.syndrome} were associated to their category on January 25th 2014. Once again this shows the peak of associations of this topic in January 2014.

At last the function \texttt{topicsInText} offers the possibilty to analyse a single documents topic allocations. The function creates a HTML document with its words colored depending on the topic allocations of each word. It requires arguments \texttt{ldaresult} and  \texttt{ldaID} as usual. The belonging \texttt{docLDA} object should be handed over in \texttt{text}, while the vocabulary set as \texttt{character} vector in \texttt{words}. You will set \texttt{id} to the documents ID you are interested in. It is possible to show the origing text by setting \texttt{originalText} to the belonging uncleared \texttt{text} component of your \texttt{textmeta} object. There are some more options - e.g. \texttt{wordOrder} - for modifying the output individually.

The article \textit{Family medicine} with ID \textit{1656748} from topic \textit{T3.health} and category \textit{Medicine} is analysed with the function \texttt{topicsInText} in more detail.
```{r, eval = FALSE}
topicsInText(text = pagesLDA, ldaresult = result, ldaID = ldaID,
  id = topArt$`2`[4,1], vocab = words5, originaltext = corpus$text, wordOrder = "")
```
\begin{center}\includegraphics[width = 0.78\textwidth]{topicText} \end{center}
In the part of the HTML output above at first the different topics in the order of its absolute appearences in the given document are displayed. The topics are represented by its 20 \texttt{top.topic.words} each and are colored each in its own color. Words which were deleted by clearing the corpus are colored black. The topic \textit{T3.health} occurs very frequently. Only two words in the extract are not allocated to this topic. The word \textit{named} belongs to the topic \textit{T10.retrieved}, the word \textit{underlying} to the topic \textit{T5.may}. This way you are able to check plausibility of individual documents, so \texttt{topicsInText} can be seen as individual cases validation as well.

# Conclusion
(TODO)
Wichtigste Punkte des Workflows zusammenfassen, allgemeine Fallstricke (Duplikate ...), Diskussion des Anwendungsbeispiels (viele stopwords nicht geloescht ...), Ausblick? (weitere Funktionen ...)

```{r, include = FALSE}
library(knitr)
purl("Vignette.Rmd", documentation = 2, quiet = TRUE)
# get R-Code of the RMD
```