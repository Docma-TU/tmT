---
title: "Package 'tmT'"
author: "Lars Koppers, Jonas Rieger, Karin Boczek, Gerret von Nordheim"
date: \today
output:
  pdf_document:
    toc: true
    number_sections: true
    # citation_package: ??
    keep_tex: true
editor_options: 
  chunk_output_type: console
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Vignette tmT}
---
\newpage
# Introduction

This package offers different functions to explore text corpora with topic models. The package focuses on the visualisation and validation of content analysis. Therefor it offers some filter for preprocessing and a wrapper for the lda from the lda-package to include a topic model.
Most visualisations aim on presentation of measures for corpora, subcorpora or topics from lda over time. To use this functionality every text needs a date specification as metadata. To harmonize different text sources we use a S3 \texttt{textmeta} object.

The current version of the package can be installed with the \texttt{devtools} package.
```{r, include = FALSE}
devtools::install_github("DoCMA-TU/tmT")
library(tmT)
```
This vignette gives an overview over the functionality of the package. For a detailed description of the functions see the help pages.

```{r, eval = FALSE}
install.packages("tmT")
library("tmT")
```

# Data Preprocessing
A basic functionality of the package is data preprocessing. Therefore several functions are given for reading text data, creating text objects, manipulating these objects and especially handling duplicates of different forms in the text data.

## Read the Corpus - \texttt{textmeta}, \texttt{readWikinews}
Read the corpus data through one of your self implemented read-functions and create a \texttt{textmeta} object with the function of the same name and the arguments \texttt{text}, \texttt{meta} and \texttt{metamult}. The \texttt{text} component should be a \texttt{list} of \texttt{character} vectors or a \texttt{list} of \texttt{lists} of \texttt{character} vectors, whereas \texttt{meta} is a \texttt{data.frame} and \texttt{metamult} is intended for mainly unstructured meta-information as a \texttt{list}. Furthermore \texttt{meta} must contain columns \texttt{id}, \texttt{date} and \texttt{title}.
You can test whether your object meets the requirements of a \texttt{textmeta} object with the function \texttt{is.textmeta}.

A read-function which is part of the package \texttt{tmT} is the function \texttt{readWikinews}. \texttt{readWikinews} reads XML-files created by the wikinews export page: [https://en.wikinews.org/wiki/Special:Export](https://en.wikinews.org/wiki/Special:Export "Wikinews Export pages"). By default \texttt{readWikinews} reads all XML-files in the working directory. The function creates a \texttt{textmeta} object. For this vignette we used two categories: *Politics_and_conflicts* and *Economy_and_business*. The pages were downloaded on 2018-03-05 in a file for each category. We can use \texttt{readWikinews} for reading both files, if they are in the same folder.

```{r, eval = FALSE}
corpus <- readWikinews()
```

Another method to read both files is to read both files seperately and merge them with the function \texttt{mergeTextmeta}. This function should be used if we want to merge data from different sources using different read-functions. We use the two example datasets from the package.

```{r, eval = TRUE}
data(politics)
data(economy)
corpus <- mergeTextmeta(list(politics, economy))
```

```{r, eval = FALSE, echo=FALSE}
save(politics, file = "../data/politics.rda")
save(economy, file = "../data/economy.rda")
```

We get a note about duplicated texts (texts that appear in both categories). We have to handle this issue later. If we merge corpora with different meta-variables we can decide if  all variables will be used for the merged corpora (\texttt{all = TRUE}, default) or only variables that appear in all corpora (\texttt{all = FALSE}).

After reading the raw data the texts need to be preprocessed.

## Remove Umlauts and XML/HTML Tags - \texttt{removeXML removeHTML removeUmlauts}

You can use \texttt{removeXML} to delete XML-tags (\texttt{<...>}) in character strings or a \texttt{list} of \texttt{character} vectors. The value you receive back will be a \texttt{character} vector or a list, if the input was a list.   
If your texts contain html entities use \texttt{removeHTML}. If you want to transform the entities in UTF-8 characters you can choose between the entity-type (\texttt{dec=TRUE}: \&#248;, \texttt{hex=TRUE}: \&#xf8; or \texttt{entity=TRUE}: \&oslash;). If you are unsure which type was used, we recommend to enable all entity-type (disadvantage: longer run time). To choose which character should be replaced you can choose from all 16 ISO-8859 lists, e.g. \texttt{symbolList=c(1,15)} for ISO-8859-1 (latin1) and ISO-8859-15 (latin9). If \texttt{delete=TRUE} all remaining entities will be deleted.
To replace german umlauts (ä ö ü ß -> ae oe ue ss) use \texttt{removeUmlauts}.

We remove XML-tags and HTML-entities from our Wikinews corpus. Since we have only punctuation as HTML-entities in the Corpus we remove it completely.
```{r}
corpus$text <- removeXML(corpus$text)
corpus$text <- removeHTML(corpus$text, dec=FALSE, hex=FALSE, entity=FALSE)
```
It is possible to apply the function to the \texttt{meta} component of a \texttt{textmeta} object as well, for example to remove XML tags or umlauts from the title of the Wikipedia pages.
```{r}
corpus$meta$title <- removeXML(corpus$meta$title)
corpus$meta$title <- removeHTML(corpus$meta$title, dec=FALSE, hex=FALSE, entity=FALSE)
```
After applying the function to the text component, we have removed all database relicts like XML-tags. At this point you should deal with identifying different types of duplicates in your text data.

## Identifying Duplicates - \texttt{deleteAndRenameDuplicates}, \texttt{duplist}
You should ensure unique IDs in all three components of your \texttt{textmeta} object. If you cannot ensure that, it is recommended to use the function \texttt{deleteAndRenameDuplicates}, which deletes "complete duplicates" - which means, that there are at least two entries with same ID and same information in \texttt{text} \textit{and} in \texttt{meta} - and renames so called "real duplicates" - at least two entries with same ID and text, but diferrent information in meta - and renames also "fake duplicates" - at least two entries with same ID but different \texttt{text} components. It is important to know that for technical reasons - expecting duplicates in the names of the \texttt{lists} - this is the only function, which works with classic indexing, so that it assumes the same order of articles in all three components.

Additionally you can identify \texttt{text} component duplicates in your corpus with the function \texttt{duplist}, which creates a \texttt{list} of different types of duplicates. Non-unique IDs are not supported by the function, which implies that \texttt{deleteAndRenameDuplicates} should be executed before.

In the given example corpus complete duplicates are only expected if pages were associated to both categories. These duplicates are deleted. 
```{r}
corpus <- deleteAndRenameDuplicates(corpus)
```
The function \texttt{deleteAndRenameDuplicates} deleted 286 complete duplicates, so that \texttt{duplist} is applicable to the corpus.
```{r}
dups <- duplist(corpus)
```
There is a possibility to visualize duplicates over time by the function \texttt{plotScot} which is explained in section 3.2.

For further analysis, especially performing the Latent Dirichlet Allocation, it is important that for each duplicate only one page is considered. Therefore it is the aim to reduce the corpus, so that it contains all pages which appear only once and a represantative page for all pages which appear twice or more frequent. In our example we have only duplicated texts containing the empty string \texttt{""} or small relicts like \texttt{"\_\_NOTOC\_\_"} or \texttt{"* * *"}

## Clean Corpus - \texttt{cleanTexts}
For further preprocessing of text corpora tosca offers the function \texttt{cleanTexts}. It removes punctuation, numbers and stopwords. By default it removes english stopwords. It uses the stopword list of the function \texttt{stopwords} from the \texttt{tm} package. For the german stopword list are some additional words implemented (e.g. "dass" and "fuer"). You can control which stopwords should be removed with the argument \texttt{sw}. In addition the function changes all words to lowercase and tokenizes the documents. The result is a \texttt{list} of \texttt{character} vectors, or if \texttt{paragraph} is set \texttt{TRUE} (default) a \texttt{list} of \texttt{lists} of \texttt{character} vectors. The sublists should represent paragraphs of a document. If you hand over a \texttt{textmeta} object instead of a \texttt{list} of texts you will receive one back. In this case you have to hand it over to the parameter \texttt{object} instead of \texttt{text}.

The examples corpus language is english, so that \texttt{sw} should be set to \texttt{stopwords()} from the \texttt{tm} package, which includes english stopwords by default (\texttt{kind = "en"}). 
```{r}
corpusClean <- cleanTexts(object = corpus)
```

The function \texttt{cleanTexts} deletes all \texttt{meta} entries which did not belong to one of the texts (e.g. deleted empty texts).
To create a \texttt{textmeta} object including this data the corresponding function is used.

```{r}
textClean2 = cleanTexts(text = corpus$text)
corpusClean2 = textmeta(text = textClean2, meta = corpus$meta)
```


## Generate Wordlist - \texttt{makeWordlist}
After cleaning the corpus with the function \texttt{cleanTexts} we are able to call the function \texttt{makeWordlist}, which creates a table of occuring words in a given corpus. THe function \texttt{table} needs much RAM. That's a problem for bis Corpora. In \texttt{makeWordlist}  we use the parameter \texttt{k} (default: \texttt{100000L}) to reduce the number of texts which are processed at once. Large \texttt{k}s lead to faster calculation but more RAM usage.

For calculating wordlists a tokenized corpus must be used. In the given example \texttt{corpusClean\$text} is handed over to the function accordingly.
```{r}
wordtable = makeWordlist(corpusClean$text)
```

# Descriptive Analysis
After preprocessing the text data there is a typical workflow we recommend for looking at the corpus. This workflow contains the generic functions \texttt{print} and \texttt{summary} as well as the highly adaptable functions \texttt{plotScot} and \texttt{plotFreq}. These graphical functions should be part of any initial analysis of text data.

## Generic Functions - \texttt{print}, \texttt{summary}
Some information about the (one to) three components of the \texttt{textmeta} object is obtained by calling the generic function \texttt{print}.
```{r}
print(corpus)
```
The function provides the count of pages in the corpus (7041) and there are two additional columns in \texttt{meta} to the mandatory ones \texttt{id}, \texttt{date} and \texttt{title}. The pages are dated from 2004-11-13 till 2018-03-04.

You will get more information, especially about counts of \texttt{NA}s and tables of some \texttt{candidates} (default: \texttt{resource} and \texttt{downloadDate}) with the generic function \texttt{summary}. In addition to \texttt{candidates} you can handover the argument \texttt{list.names} (default: \texttt{names(object)}) for specifying the components out of \texttt{text}, \texttt{meta} and \texttt{metamult} which should be analysed by the function.
```{r}
summary(corpus)
```
Apparently there are 191 \texttt{NA}s in the variable \texttt{date}.

## Visualisation of Corpus over Time - \texttt{plotScot}
One of the descriptive plotting functions in the package is \texttt{plotScot} (\textbf{S}ub\textbf{C}orpus\textbf{O}ver\textbf{T}ime) which creates a plot of counts or proportion of either documents or words in a (sub)corpus over time. The subcorpus is specified by \texttt{id} and it is possible to set the \texttt{unit} to which the dates should be floored (default: \texttt{"month"}). The argument \texttt{curves = c("exact", "smooth", "both")} determine which curve(s) should be plotted. If you select \texttt{type = "words"}, the object which you handover should be a tokenized \texttt{textmeta} object. If \texttt{type = "docs"} (default) you can hand over untokenized \texttt{textmeta} object as well.

First of all the number of texts per month in the complete example corpus is plotted, as exact and smoothed curve.
```{r, fig.width=5, fig.height=2.8, fig.align=2}
plotScot(corpusClean, curves = "both")
```
The black curve is the exact one and the red curve represents the smoothed values. The grafic gives a first impression about the distribution of the texts over time. Most of the news articles where written between 2005 and 2009. 
If we want to identify the distribution of duplicates over the time we can use \texttt{plotScot} to plot the IDs of the not duplicated texts in the corpus. 
```{r, fig.width=5, fig.height=2.8, fig.align=2}
plotScot(corpus, id = dups$notDuplicatedTexts, rel = TRUE)
```
The plot shows that between 2006 and 2011 around 80 per cent of the corpus are not duplicated texts. Most zeros in the plot result from no articles in the whole corpus during these time periods. It is possible to get these values as \texttt{NAs} by setting \texttt{natozero = FALSE} in \texttt{plotScot}. This option works if \texttt{rel = TRUE} and is offered by many other functions in the package. Usually all plot functions in the package return the data belonging to the plot as invisible output. These plot functions offer a lot more functionality, which is described in the corresponding help functions.

## Frequency Analysis - \texttt{plotFreq}
The other descriptive plotting function is \texttt{plotFreq} which performs a frequency analysis. Most of the arguments does not differ from \texttt{plotScot}. But the options \texttt{wordlist} and \texttt{link = c("and", "or")} are added for specifying the words of the frequency analysis and their link within one vector. In detail \texttt{wordlist} could either be a \texttt{list} of \texttt{character} vectors or a single \texttt{character} vector, which will be coerced to a \texttt{list} of the vector's length. Each \texttt{list} entry represents a set of words which all (default \texttt{link = "and"}) or one of them (\texttt{link = "or"}) should appear in an article to be counted. The function uses \texttt{filterWord} with \texttt{out = "count"}, which is explained later on, for counting.

The example corpus contains Wikinews articles concerning the categories *Politics_and_conflicts* and *Economy_and_business*. Therefore some typical words out of these categories were taken to perform a frequency analysis. First of all the words \textit{unemployment}, \textit{growth} and \textit{trade} were taken. \colorbox{red}{The function identifies patterns. Geht auch word? Trunkierung wie bei filterWord? ... Nein geht nicht!} 
\colorbox{red}{TODO:[Grafiken: Link-Beschreibung in Grafiküberschrift immer noch nicht perfekt zu sehen. Vorschlag: "over time. Link:" statt "over time - link:" evtl. reicht das schon ]}
```{r, fig.width=5, fig.height=3.5, fig.align=2, fig.show="hold"}
wordsEconomy <- list("unemployment", "growth", "trade", c("unemployment", "growth", "trade"))
plotFreq(corpusClean, wordlist = wordsEconomy, curves = "smooth",
  ylim = c(0, 25), legend = "topright")
plotFreq(corpusClean, wordlist = wordsEconomy, link = "or", curves = "smooth",
  ylim = c(0, 25), legend = "topright")
```
In the figures above you can see the difference between the \textit{and} link and \textit{or} link. In the first figure the three curves indicate the single words. The fourth curve shows the number of texts in which all three words appear. For most dates no texts meets this requirement. In the second figure the same single curves are shown. The fourth curve represents all three words again, but setting \texttt{link = "or"}. The curve lies above the three others in every point. Due to smoothing it is possible that the line falls under one of the single word lines. This can be avoided by choosing \texttt{curves = "exact"}. 

In another figure the counts of pages in which the words \textit{crisis}, \textit{war} and \textit{conflict} appear, are analysed. You can see that it is often useful to compare smoothed and exact curves to visualize the variance and a trend in the data.
```{r, fig.width=5, fig.height=3, fig.align=2}
plotFreq(corpusClean, wordlist = list(c("crisis", "war", "conflict")), link = "or",
  curves = "both", both.lwd = 2, legend = "topright")
```

## Write CSV Files - \texttt{showTexts}, \texttt{showMeta}
There are two functions for writing csv files implemented in the package. Both needs an \texttt{textmeta} object in \texttt{showTexts}, respectively the \texttt{meta} component of any-formated \texttt{textmeta} object in \texttt{showMeta}. The default of the parameter \texttt{id} in \texttt{showTexts} are all document IDs of the corpus as a \texttt{character} vector, but it is possible to handover a \texttt{character} matrix as well, so that each column will be represented in seperated csv file. In the first column of the csv file there will be the ID of each document, in the second and third the title and the date, whereas in the fourth column there will be the text itself.

Six IDs are sampled from the whole corpus with a given seed. These pages are saved as \texttt{corpus1lesen.csv} and are returned as invisible to \texttt{temp}.
```{r}
set.seed(123)
id <- sample(corpus$meta$id, 6)
temp <- showTexts(corpus, id = id)
temp[, c("id", "date", "title")]
```

We now have a look at the meta data. The default of the parameter \texttt{id} in \texttt{showMeta} are the IDs which are in the column \texttt{meta\$id}. You can also handover a matrix of IDs like in \texttt{showTexts} and you can specify which columns of the \texttt{meta} component you want to be written in the csv by setting the argument \texttt{cols} (default: \texttt{colnames(meta)}).

Analogously to \texttt{showTexts} the following code example will create three files named \texttt{corpus<i>meta.csv}, where $i = 1,2,3$ stands for the $i$-th column of the matrix of IDs.
```{r}
temp <- showMeta(corpus$meta, id = matrix(id, nrow = 2),
  cols = c("title", "date"))
temp
```

# Generating Subcorpora
The preprocessing which was done before is mandatory. For further preparation the package offers functions for filtering the corpus by dates, wordcount or search terms to generate subcorpora.

## Filter Corpus by Dates - \texttt{filterDate}
There are three implemented ways to filter your text corpora: One of these is the function \texttt{filterDate}, which filters a given \texttt{textmeta} object by a time period. The function works on any formated objects of class \texttt{textmeta} and extracts documents out of the \texttt{text} component, from which the date column in the \texttt{meta} component is in between \texttt{s.date} and \texttt{e.date} - including texts from both dates. The return value is the filtered \texttt{textmeta} object or a \texttt{list} which could be the \texttt{text} component of a \texttt{textmeta} object respectively, if you hand over the \texttt{text} and \texttt{meta} component not as a \texttt{textmeta} object.

The example corpus is filtered to articles dated between 2006 and 2009.
```{r}
corpusDate <- filterDate(corpusClean, s.date = "2006-01-01", e.date = "2009-12-31")
print(corpusClean)
print(corpusDate)
```
The filtered corpus contains only the 3909 texts from the period 2006 till 2009. 

## Filter Corpus by Wordcount - \texttt{filterCount}
\colorbox{red}{TODO:(folgt...)}

## Filter Corpus by Words - \texttt{filterWord}
The use of \texttt{filterWord} works analogously. It filters the \texttt{text} component of a \texttt{textmeta} object by appearances of specific words. The function uses regular expressions. It filters the given documents in the \texttt{text} component by words handed over by \texttt{search}, which could be a simple \texttt{character} vector or a \texttt{list} of \texttt{data.frames}. In the case of a character vector handed over the entries of the vector are linked by an \textit{or}, so if \textit{any} of the words appears in one specific document, it is returned.

Maybe you are not interested in the texts of the documents itself. Therefore you can set \texttt{out} to control the output: By default (\texttt{out = text}) you receive the filtered documents or if you hand over the argument \texttt{object} the corresponding \texttt{textmeta} respectively. If you choose \texttt{out = bin} you get the corresponding logical vector of indices and if you choose \texttt{out = count} you get a matrix - with the number of documents rows and the \texttt{search}-length respectively vector-length columns - which indicates in row \textit{i} and column \textit{j} how often the \textit{j}-th word of the \texttt{wordlist} appears in the according \textit{i}-th document.

First of all small examples are given for understanding the functionality of the function \texttt{filterWord}. An examples for the \textit{or}-link is given by the next code example.
```{r}
toyCorpus <- list(text1 = "dataset", text2 = "anything")
searchterm <- c("data", "as", "set", "anything")
filterWord(text = toyCorpus, search = searchterm, out = "bin")
```
The returned values are \texttt{TRUE} twice. There is at least one pattern in the \texttt{searchterm} vector which appears at least once in each of the strings \textit{dataset} and \textit{anything}.

In the case of a \texttt{list} of \texttt{data.frames} handed over each \texttt{data.frame} is linked by an \textit{or} and should contain columns \texttt{pattern}, \texttt{word} and \texttt{count}. The parameter \texttt{pattern} includes the search terms, the column \texttt{word} is a logical variable which controls whether words (\texttt{TRUE}) or patterns are searched. Alternatively \texttt{word} can be a \texttt{character} string containing the keyword \texttt{left} or \texttt{right} for left- or right-truncated search, i.e. word = right searches for the exact pattern on the left of the word and all possible endings of the pattern. You must set the argument \texttt{count} to an \texttt{integer}. This argument controls how often a word or pattern must appear in a document for it to be returned. Rows in each \texttt{data.frame} are linked by an \textit{and}. An example is given by the following code.
```{r}
searchframe <- data.frame(pattern = searchterm, word = FALSE, count = 1)
filterWord(text = toyCorpus, search = searchframe, out = "bin")
```
In the case that \texttt{words} is handed over as \texttt{data.frame}, the \textit{and} link is active. The function checks whether all of the patterns appear as part of words in the two entries of \texttt{texts}. Therefore the function returns \texttt{FALSE} twice.

For another sample case, we will delete the word \textit{anything} from the search terms.
```{r}
filterWord(text = toyCorpus, search = searchframe[1:3,], out = "bin")
```
If you omit the word \textit{anything} from \texttt{searchterm} you receive a \texttt{TRUE} for text1 (\textit{dataset}) - all three patterns appear in it - and a \texttt{FALSE} for text2 (\textit{anything}), because not all patterns appear in it, not even one of them.

An example of \texttt{out = count} to receive a count for each text and search term combination is given by the following.
```{r}
filterWord(text = list(text1 = c("i", "was", "here", "text"),
  text2 = c("some", "text", "about", "some", "text", "and", "something", "else")),
  search = c("some", "text"), out = "count")
```
In the case of \texttt{out = count} it is useful, that \texttt{search} is a simple \texttt{character} vector.

Another application of \texttt{filterWord} is to apply the function with \texttt{word = TRUE}, so that the function searchs only for single words, not for strings containing these words. This is displayed by the following example.
```{r}
searchterm <- list(text1 = "land and and", text2 = c("and", "land", "and", "and"))
searchframe <- list(
  data.frame(pattern = "and", word = FALSE, count = 1),
  data.frame(pattern = "and", word = TRUE, count = 1))
filterWord(text = searchterm, search = searchframe, out = "count")
```
The function returns counts \texttt{c(3, 4)} for the simple pattern search and \texttt{c(2, 3)} for the word search, because the word \textit{and} appears once in every document of \texttt{searchterm} only as pattern and not as single word.

After understanding the functionality of the function, finally it is used for filtering the Wikipedia corpus. The example corpus is filtered to those pages which fit to the chosen categories sofar, that the name of one of the catagories should appear on the page at least once, at least as pattern. It is not necessary to set \texttt{ignore.case} because the Wikipedia corpus was cleaned before. This step includes that all words are lowercase now. \colorbox{red}{TODO:[AND OR, OR Logik] ... wurde ja vorher schon beschrieben}
```{r}
searchterm <- list(
  data.frame(pattern = "economy", word = FALSE, count = 1),
  data.frame(pattern = c("world", "economy"), word = FALSE, count = 1),
  data.frame(pattern = "politics", word = FALSE, count = 1))
corpusFiltered = filterWord(corpusDate, search = searchterm)
print(corpusDate)
print(corpusFiltered)
```
The date and word filtered corpus consists of 451 texts compared to 3909 texts in the original \texttt{corpusDate} corpus.

# Latent Dirichlet Allocation
The main analytical functionality requested by text mining tools is to perform and analyse a Latent Dirichlet Allocation. In the package \texttt{tmT} this is ensured by the function \texttt{LDAgen} for performing the LDA, functions for validating the LDA results and various functions to visualize the results in different ways, especially over time. It is possible to analyse individual articles and its topic allocations as well. In addition a function for preparing your corpus for performing a Latent Dirichlet Allocation is given. This function creates a object which can be handed over to the function you could use for a LDA.

## Transform Corpus - \texttt{LDAprep}
The last step before performing a Latent Dirichlet Allocation is to create corpus data, which could be handed over to the function \texttt{lda.collapsed.gibbs.sampler} from the \texttt{lda} package or the function \texttt{LDAgen} from this package respectively. This is done by using the function \texttt{LDAprep} with its arguments \texttt{text} (\texttt{text} component of a \texttt{textmeta} object) and \texttt{vocab} (\texttt{character} vector of vocabularies). These vocabularies are the words which are taken into account for LDA.

You can have a look at the documentation of the \texttt{lda.collapsed.gibbs.sampler} for further information about lda. The function  \texttt{LDAprep} offers options \texttt{ldacorrect}, \texttt{excludeNA} and \texttt{reduce} set all \texttt{TRUE} by default. The returned value is a \texttt{list} in which every entry symbolizes an article and contains a matrix with two rows. In the first row there is the index of each word in \texttt{vocab} minus one, in the second row there is the number of appearances of each word in the article. The option \texttt{ldacorrect = TRUE} ensures the second row is always one and the number of the appearances of the word will be shown by the number of columns belonging to this word. This structure is needed by \texttt{lda.collapsed.gibbs.sampler}.

Looking at the example corpus at first a new wordlist must be generated based on the filtered corpus.
```{r}
wordtableFiltered <- makeWordlist(corpusFiltered$text, method = "radix")
```
```{r}
head(sort(wordtableFiltered$wordtable, decreasing = TRUE))
```

```{r}
words5 <- wordtableFiltered$words[wordtableFiltered$wordtable > 5]
pagesLDA <- LDAprep(text = corpusFiltered$text, vocab = words5)
```
After receiving the words which appear at least six times in the whole filtered corpus, the function \texttt{LDAprep} is applied to the example corpus with \texttt{vocab = words5}. The object \texttt{pagesLDA} will be handed over to the function which performs a Latent Dirichlet Allocation.

## Performing LDA - \texttt{LDAgen}
The function which has to be applied first to the corpus manipulated by \texttt{LDAprep} is \texttt{LDAgen}. Therefore the function offers the options \texttt{K} (\texttt{integer}, default: \texttt{K = 100L}) to set the number of topics, \texttt{vocab} (\texttt{character} vector) for specifying the words which are considered in the manipulation of the corpus and several more e.g. number of iterations for the burnin (default: \texttt{burnin = 70}) and the number of iterations for the gibbs sampler (default: \texttt{num.iterations = 200}). The result will be saved in a \texttt{R} workspace, the first part of the results name can be specified by setting the option \texttt{folder} (default: \texttt{folder = "lda-result"}).

In the concrete example corpus the manipulated corpus \texttt{pagesLDA} is used for \texttt{documents}, the topic number is set to \texttt{K = 10} and for reproducibility a seed is set to \texttt{seed = 123}. The filename consists of the \texttt{folder} argument followed by the options of \texttt{K}, \texttt{num.iterations}, \texttt{burnin} and the \texttt{seed} of the LDA.
```{r, eval = TRUE}
LDAgen(documents = pagesLDA, K = 10L, vocab = words5, seed = 123)
load("lda-result-k10i200b70s123alpha0.1eta0.1.RData")
```

<!--  ```{r, include = FALSE} -->
<!-- load("lda-result-k10i200b70s123alpha0.1eta0.1.RData") -->
<!-- ``` -->
For validation of the LDA result and further analysis, the result is loaded back to workspace.

## Validation of LDA Results - \texttt{intruderWords}, \texttt{intruderTopics}
For validation of LDA results there are two functions in the package. These functions expect user input, the user works like a text labeller. The LDA result is handed over by setting \texttt{beta = result\$topics}. During the function \texttt{intruderWords} the labeler gets a set of words. The number of words can be set by \texttt{numOutwords} (default: 5). This set represents one topic. It includes a number of intruders (default: \texttt{numIntruder = 1}), which can also be zero. In general, if the user identifies the intruder(s) correctly this is an identifier for a good topic allocation. You can set options \texttt{numTopwords} (default: 30) to control which top words of each topic are considered for this validation. In addition it is possible to enable or disable the possibility for the user to mark nonsense topics. By default this option is enabled (\texttt{noTopic = TRUE}). The true intruder can be printed to the console after each step with \texttt{printSolution = TRUE} (default: \texttt{FALSE}).
\colorbox{red}{TODO:[non-preprocessed corpus nach removexml ... verstehe den Punkt nicht.]}

The LDA result of the example corpus is checked by \texttt{intruderWords} with a number of intruders of zero or one.
```{r, eval = FALSE}
set.seed(173)
intWords <- intruderWords(beta = result$topics, numIntruder = 0:1)
```
```{r, include = FALSE}
set.seed(173)
intWords <- intruderWords(beta = result$topics, numIntruder = 0:1,
  test = TRUE, testinput = as.character(c(5,0,0,5,1,2,0,5,3,5)))
```
```{r, echo = FALSE}
set.seed(173)
toDelete <- intruderWords(beta = result$topics, numIntruder = 0:1,
  test = TRUE, testinput = "q", printSolution = TRUE)
```
As an illustration the first set is shown. The word \textit{will} does not fit into the set with the words \textit{budget}, \textit{government}, \textit{tax} and \textit{billion}. Therefore the user would type \texttt{5} and press enter. If the user wants to mark nonsense topics he would type an \texttt{x} (in the summary the number of meaningful topics is shown) and \texttt{0} if he thinks there is no intruder word. Actually \textit{will} is the true intruder in the set above. As an example user input \texttt{c(5, 0, 0, 5, 1, 2, 0, 5, 3, 5)} is considered.
```{r}
print(intWords)
```
By printing the object of \texttt{intruderWords} to the console, you get information about options for the validation strategy and a results matrix with ten rows an three columns. The rows indicate the different sets of potential intruders. For each set the matrix contains information how many intruder are in the specific set, how many intruders were missed by the user and how many false intruders were named. Of course the columns \texttt{missIntr} und \texttt{falseIntr} matchs if \texttt{numIntruder} is a scalar and the user names exactly this number of potential intruders for each set.
```{r}
summary(intWords)
```
Applying \texttt{summary} to an object of type \texttt{intruderWords} will result in an ouput of some measures concerning the validation. Each function call contains ten sets. You are able to continue labelling by calling \texttt{intruderWords} with \texttt{oldResult = intWords} if your set was not finished.
```{r, eval = FALSE}
intWords <- intruderWords(oldResult = intWords)
```

Analogously to \texttt{intruderWords} you can use \texttt{intruderTopics} for validation the other way around. This function is used for validation of topics associated to a specific document instead of validation of words associated to one topic. Therefore the document is displayed in another window and a sample of topics - represented by the ten \texttt{top.topic.words} - is shown in the console. You should hand over in \texttt{text} the text component of the original untokenized corpus before manipulation by \texttt{cleanTexts}, so that the document is readable. The user then names the intruder(s). There are options for different numbers of topics and intruders as in \texttt{intruderWords} as well. The parameter \texttt{theta} should be set to \texttt{result\$document\_expects} given \texttt{result} is the LDA result. An example call is given below.
```{r, eval = FALSE}
intruderTopics(text = corpus$text, id = ldaID,
  beta = result$topics, theta = result$document_expects)
```

## Clustering of Topics - \texttt{clusterTopics}, \texttt{mergeLDA}
For analysing topic similarities it is useful to cluster the topics. The function \texttt{clusterTopics} implements this. The main argument is \texttt{topics} and should be set to the \texttt{topics} element of the \texttt{result} object. You could specify \texttt{file}, \texttt{width} and \texttt{height} (both \texttt{integers}) to write the resulting plot to a pdf. Other options are \texttt{topicnames} for labelling the topics in the plot and \texttt{method} (default: \texttt{"average"}), which influences the way the topics are clustered. The \texttt{method} statement is used for applying the distance matrix to the function \texttt{hclust}. The distance matrix is computed based on the hellinger distance and is returned in a list together with the value of the \texttt{hclust} call as invisible by \texttt{clusterTopics}.

```{r, fig.width=5, fig.height=3.3, fig.align=2}
clustRes <- clusterTopics(ldaresult = result, xlab = "Topic", ylab = "Distance")
names(clustRes)
```
The same plot as above can be recreated by calling \texttt{plot(clustRes\$cluster)}. In the plot you can see the similarities concerning the hellinger distance of the topics. For example the plot hints to the similarity with regards to content between topic 2 and topic 9, which both contain economic terms with topic 9 focussing more on economic policy. Topic 7 and 10 both include words on Canadian politics.   

It is possible to merge different results of LDAs by calling \texttt{mergeLDA(list(result1, result2, ..., resultN))}. The function \texttt{mergeLDA} binds the \texttt{topics} elements of the results by row and only consider words which appears in all results. As result you receive the \texttt{topics} matrix including all topics from the given results.

## Visualisation of Topics over Time - \texttt{plotTopic}
As extension of the highly flexible functions \texttt{plotScot} and \texttt{plotFreq} the package \texttt{tmT} offers another plotting function of the same type. The function \texttt{plotTopic} does something very similar to these two functions. It pictures the counts or proportion of words allocated to different topics of a LDA result over time. The result object is handed over in \texttt{ldaresult}, the belonging IDs of documents as a \texttt{character} vector in \texttt{ldaid}. In \texttt{object} the function expect a strictly tokenized \texttt{textmeta} object. You could set \texttt{select} for selecting topics by an \texttt{integer} vector. By default all topics are selected. Analoguesly to \texttt{wnames} in \texttt{plotFreq} it is possible to set topic names with \texttt{tnames}. By default the index and the most representative word (\texttt{top.topic.words}) per topic are chosen as names. For further individualisation the function offers mostly the same options as \texttt{plotScot} and \texttt{plotFreq}.

Often it is useful to choose \texttt{curves = "smooth"} if you do not select topics, because there is a massive fluctuation of exact curves. However, it is important to have a look at the exact curves, because the smoothed curves are someway manipulated by the statement \texttt{smooth}, so the user is tempted to optimise the smoothing parameter for getting the curves he or she wants.
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
plotTopic(object = corpusFiltered, ldaresult = result, ldaID = ldaID,
  rel = TRUE, curves = "smooth", smooth = 0.1, legend = "none", ylim = c(0, 0.7))
```

There is no difference of handing over an inflated corpus with documents which were not used for LDA. But the corpus must contain all documents of the LDA.
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
plotTopic(object = corpusClean, ldaresult = result, ldaID = ldaID,
  select = c(2, 5, 8:9), rel = TRUE, curves = "both", smooth = 0.1)
```

## Visualisation of Topic Share over Time - \texttt{plotArea}
The function \texttt{plotArea} offers possibilities to create so called area visualisations of topics over time. It requires arguments \texttt{ldaresult}, \texttt{ldaid} and \texttt{meta} as introduced before. There are options \texttt{select}, \texttt{tnames}, \texttt{unit} and others. Additionally you can set \texttt{threshold} to a \texttt{numeric} between 0 and 1, as a limit, which a topics proportion have to surpass at least once to be plotted.

Because this seems to be interesting topics \colorbox{red}{Inhaltlich checken!} \textit{T2.economy} (blue curve), \textit{T9.economic policy} (green) and \textit{T5.US politics} (red) are plotted in a sediment plot. The chosen \texttt{unit} is \texttt{"bimonth"} (default is \texttt{"quarter"}).
```{r, eval = FALSE}
plotArea(ldaresult = result, ldaID = ldaID, meta = corpusFiltered$meta,
  select = c(2, 9, 5), unit = "bimonth", sort = FALSE)
```
```{r, echo=FALSE, fig.width=5.5, fig.height=3, fig.align=2}
par(mar = c(3.5,3,1.5,1.5)+0.1)
plotArea(ldaresult = result, ldaID = ldaID, meta = corpusFiltered$meta,
  select = c(2, 9, 5), unit = "bimonth", sort = FALSE)
```
\colorbox{red}{(Inhaltlich checken!)} Examplary interpretation: The topic \textit{T2.economy} increases over time, especially after the bank Lehman Brothers declared bankruptcy in September 2008, the starting point of the global financial crisis. \textit{T5.US politics} is considerably larger during the 2008 presidential election campaign.

## Visualisation of Words in Topic over Time - \texttt{plotTopicWord, plotWordpt}
Another visualise topics over time is given by \texttt{plotTopicword}. It displays the counts or proportion of given topic-word combinations. If \texttt{rel = TRUE} the baseline for normalisation are the words counts, not the counts of topics. Arguments which have to specified are \texttt{object} (corpus, \texttt{textmeta} object), \texttt{docs} (corpus manipulated by \texttt{LDAprep}, the input for \texttt{LDAgen}) and the \texttt{ldaresult} with its \texttt{ldaid} (IDs of documents in \texttt{docs} or \texttt{ldaresult} respectively). The function asks for \texttt{docs} for complexity reasons. This object should be created while preparation for LDA anyway. The options \texttt{wordlist} and \texttt{select} are known from other plot functions and offer a lot of different topic word combinations which should be plotted by \texttt{plotTopicword}.

In the example corpus the proportion of the word \textit{economy} in the topics one, three and seven is explored. The \texttt{top.topic.words} of the three chosen topics are \colorbox{red}{Inhaltlich checken!}\textit{percent} (T2.economy, lightgreen curve), \textit{obama} (T5.US politics, orange) and \textit{tax} (T9.economic policy, purple).
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
plotTopicWord(object = corpusFiltered, docs = pagesLDA, ldaresult = result, ldaID = ldaID,
  wordlist = "economy", select = c(2, 5, 9), rel = TRUE, legend = "topleft")
```
The graphic shows that the word \textit{economy} is associated to the topic \colorbox{red}{\textit{T2.economy, lightgreen curve}} most often.

For interpretating it is important to keep in mind the baseline, the word counts of \textit{economy}. To display this the sums of all topic-word proportions are calculated and are expected to be one for all dates which appear at least once, otherwise zero.
```{r, eval = FALSE}
tab <- plotTopicWord(corpusFiltered, pagesLDA, result, ldaID, "economy", rel = TRUE)
all(round(rowSums(tab[, -1]), 10) %in% c(1, 0))
```
```{r, include = FALSE}
tab <- plotTopicWord(corpusFiltered, pagesLDA, result, ldaID, "economy", rel = TRUE)
all(round(rowSums(tab[, -1]), 10) %in% c(1, 0))
```
```{r, echo = FALSE}
all(round(rowSums(tab[, -1]), 10) %in% c(1, 0))
```
This is confirmed by the call above. For some analysis maybe it could be interesting to take the other possible baseline, the topic counts, into account. Therefore there is an additional function called \texttt{plotWordpt}.

The function \texttt{plotWordpt} works analogously like its pendant \texttt{plotTopicWord}, but with baseline topic sums instead of word sums. The difference between both functions \texttt{plotWordpt} and \texttt{plotTopicWord} is given by the fact that \texttt{plotWordpt} considers topic peaks. You will get the relative counts of the selected word(s) in the selected topic(s). All curves sum up to one if you choose any topic and the whole vocabulary list as wordlist.
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
plotWordpt(object = corpusFiltered, docs = pagesLDA, ldaresult = result, ldaID = ldaID,
  wordlist = "economy", select = c(2, 5, 9), rel = TRUE)
```

## Visualisation of Words in Articles allocated to Topics - \texttt{plotWordSub}
Imagine you want to identify words which are used frequently in articles allocated to a topic. The function which realizes a plot to the question is called \texttt{plotWordSub}. The first problem is allocation of topics. Therefore you set a absolute or relative limit how often words of a given article are allocated to one topic. Additionally you have to specify whether one article is allocated exactly once, maximum once or multiple times depending on the \texttt{limit} argument. The default is \texttt{limit = 10} and \texttt{alloc = "multi"}, so an article is allocated to a topic if it contains at least 11 words which are allocated to the given topic. Multiple or no allocations are possible. After allocating the articles to the topics the function creates subcorpora using \texttt{filterWord}. To control the filter you have to set the \texttt{search} argument. The counts of the subcorpora (normalized to their whole corpora) are plotted. There are many options to personalize your plot like in the other plot functions.
```{r, fig.width=5.5, fig.height=3.5, fig.align=2}
searcheco <- data.frame(pattern = "economy", word = TRUE, count = 3)
plotWordSub(object = corpusFiltered, ldaresult = result, ldaID = ldaID, limit = 1/3,
  select = c(2, 5, 9), search = searcheco, unit = "quarter", legend = "topright")
```
The plot shows subcorpora generated by the \texttt{search} argument above, which means articles must contain the word \textit{economy} at least three times. The corpora from which these subcorpora are generated have to contain one third of words which are allocated to the corresponding topic (\texttt{limit = 1/3}). 

## Heatmap of Topics over Time including Clustering - \texttt{plotHeat}
The use case for \texttt{plotHeat} is given by searching for explicit peaks of coverage of some topics. Therefore the resulting heatmap shows the deviation of the proportion of a given topic at this current time from its mean proportion. In addition a dendrogramm is plotted on the left side of the heatmap showing similarities of topics. The clustering is performed with \texttt{hclust} on the dissimilarities computed by \texttt{dist}.

By default the  proportions are calculated on the article lengths, but it is possible to force calculation on only the LDA vocabulary by setting \texttt{object} to a \texttt{textmeta} object only including meta information. Otherwise a strictly tokenized \texttt{textmeta} object is required. The parameters \texttt{ldaresult} and \texttt{ldaID} expect a LDA result and according IDs like in functions mentioned before. Options \texttt{tnames} (topic label), \texttt{file} (if you want to save the plot in a pdf) and \texttt{unit} (default: round dates to \texttt{"year"}) are given as well. Additionally it is possible to set whether the deviations should be normalised to take different topic sizes into account (default: \texttt{norm = FALSE}). You can change the intervals of labeling on the x-axis by setting \texttt{date\_breaks}. By default (\texttt{date\_breaks = 1}) every label is drawn. If you choose \texttt{date\_breaks = 5} every fifth label will be drawn.

The increase of the topic \colorbox{red}{\textit{T2.economy} after September 2008} was mentioned before. This should be visible in the following heatmap as well. As compromise between clarity and interpretability \texttt{unit = "quarter"} is chosen.
```{r, fig.height=6, fig.width=10}
plotHeat(object = corpusFiltered, ldaresult = result, ldaID = ldaID, unit = "quarter")
```
\colorbox{red}{As expected the \textit{T2.economy}} topic increase is Cleanly identifiable. The according rectangles are colored more and more red from the first quarter of 2009 in this figure. On the other hand mostly all other quarters of years concerning this topic are colored lightblue. Other remarkable quarters is for example the third quarter of 2007, where the topic \textit{T10.Canadian politics} has a noticeable peak. The dendrogramm shows that the topics are not very similar to another concerning the absolute deviations of topic proportion from the mean topic proportion per quarter. This supports the findings of clustering the topics with \texttt{clusterTopics}.

## Individual Cases Contemplation - \texttt{topTexts}, \texttt{topicsInText}
Sometimes it is useful to look at some individual cases sometimes. Especially the documents with the highest counts or proportion of words belonging to one topic are of interest. These documents can be extracted by \texttt{topTexts}. By default (\texttt{rel = TRUE}) the proportion is considered. The function requires a \texttt{ldaresult} and the according \texttt{ldaid}. It offers options \texttt{select}, \texttt{limit} and \texttt{minlength}, which control how much articles per topic (default: all topics) are returned (default: \texttt{limit = 20}) and articles of which minimum length (default: \texttt{minlength = 30}) are taken into account. The output value is a matrix of the according IDs.

In the example the top four pages from the topics \colorbox{red}{\textit{T2.economy}, \textit{T5.US politics} and \textit{T9.economic policy}} are requested.
```{r}
topID <- topTexts(ldaresult = result, ldaID = ldaID, select = c(2, 5, 9), limit = 4)
dim(topID)
```
Obviously the corresponding matrix has four rows and three columns.

After identifying the top pages it is possible to have a deeper look at these articles. Therefore the mentioned function \texttt{showTexts} can be used. The returned value is a list with three entries with \texttt{data.frames} of four rows - the different pages - and four columns each - \textit{id}, \textit{title}, \textit{date} and \textit{text}. For displaying, the fourth column of each \texttt{data.frame} containing the pages content itself is removed.
```{r}
topArt <- showTexts(corpusFiltered, id = topID)
lapply(topArt, function(x) x[, 1:3])
```

At last the function \texttt{topicsInText} offers the possibilty to analyse a single document's topic allocations. The function creates a HTML document with its words colored depending on the topic allocations of each word. It requires arguments \texttt{ldaresult} and  \texttt{ldaID} as usual. The belonging \texttt{LDAprep} object should be handed over in \texttt{text}, while the vocabulary set as \texttt{character} vector in \texttt{words}. You will set \texttt{id} to the documents ID you are interested in. It is possible to show the origing text by setting \texttt{originalText} to the belonging uncleaned \texttt{text} component of your \texttt{textmeta} object. There are some more options - e.g. \texttt{wordOrder} - for modifying the output individually.

\colorbox{red}{TODO: Soll TopicsInText-Beispiel rein?}The article \textit{Family medicine} with ID \textit{1656748} from topic \textit{T3.health} and category \textit{Medicine} is analysed with the function \texttt{topicsInText} in more detail.
```{r, eval = FALSE}
topicsInText(text = pagesLDA, ldaresult = result, ldaID = ldaID,
  id = topArt$T3.economy[4,1], vocab = words5, originaltext = corpus$text, wordOrder = "")
```
<!-- \begin{center}\includegraphics[width = 0.78\textwidth]{topicText} \end{center} -->
In the part of the HTML output above at first the different topics in the order of its absolute appearences in the given document are displayed. The topics are represented by its 20 \texttt{top.topic.words} each and are colored each in its own color. Words which were deleted by cleaning the corpus are colored black. This way you are able to check plausibility of individual documents, so \texttt{topicsInText} can also be seen as individual case validation.

# Conclusion
(TODO)
Wichtigste Punkte des Workflows zusammenfassen, allgemeine Fallstricke (Duplikate ...), Diskussion des Anwendungsbeispiels (viele stopwords nicht geloescht ...), Ausblick? (weitere Funktionen ...)

```{r, include = FALSE}
library(knitr)
purl("Vignette.Rmd", documentation = 2, quiet = TRUE)
# get R-Code of the RMD
```